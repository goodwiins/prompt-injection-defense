{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# üìä Prompt Injection Detection Benchmark\n",
    "\n",
    "Interactive notebook for benchmarking prompt injection detection against standardized public datasets.\n",
    "\n",
    "**Datasets:**\n",
    "- üî¥ **SaTML CTF 2024** - Real adversarial attacks from competition\n",
    "- üü° **deepset/prompt-injections** - Diverse injection attempts\n",
    "- üü¢ **NotInject** - Benign samples with trigger words (over-defense testing)\n",
    "- üîµ **LLMail-Inject** - Email-based injection scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mJupyter Server crashed. Unable to connect. \n",
      "\u001b[1;31mError code from Jupyter: 1\n",
      "\u001b[1;31mTraceback (most recent call last):\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/bin/jupyter-notebook\", line 3, in <module>\n",
      "\u001b[1;31m    from notebook.app import main\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/notebook/app.py\", line 18, in <module>\n",
      "\u001b[1;31m    from jupyter_server.serverapp import flags\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jupyter_server/serverapp.py\", line 40, in <module>\n",
      "\u001b[1;31m    from jupyter_events.logger import EventLogger\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jupyter_events/__init__.py\", line 3, in <module>\n",
      "\u001b[1;31m    from .logger import EVENTS_METADATA_VERSION, EventLogger\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jupyter_events/logger.py\", line 15, in <module>\n",
      "\u001b[1;31m    from jsonschema import ValidationError\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jsonschema/__init__.py\", line 13, in <module>\n",
      "\u001b[1;31m    from jsonschema._format import FormatChecker\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jsonschema/_format.py\", line 11, in <module>\n",
      "\u001b[1;31m    from jsonschema.exceptions import FormatError\n",
      "\u001b[1;31m  File \"/Users/goodwiinz/.gemini/antigravity/scratch/prompt-injection-defense/.venv/lib/python3.14/site-packages/jsonschema/exceptions.py\", line 14, in <module>\n",
      "\u001b[1;31m    from attrs import define\n",
      "\u001b[1;31mModuleNotFoundError: No module named 'attrs'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path so local modules import correctly\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "import structlog\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure Logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.dev.ConsoleRenderer()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom Imports\n",
    "from benchmarks import (\n",
    "    BenchmarkRunner, \n",
    "    # specific loaders not strictly needed if BenchmarkRunner handles them, \n",
    "    # but good to have if you want to inspect data manually later\n",
    "    load_satml_dataset, \n",
    "    AVAILABLE_DATASETS \n",
    ")\n",
    "from src.detection.embedding_classifier import EmbeddingClassifier\n",
    "\n",
    "# --- Configuration ---\n",
    "MODELS_DIR = Path('models')\n",
    "\n",
    "# Find models\n",
    "model_files = sorted(list(MODELS_DIR.glob('*_classifier.json')))\n",
    "\n",
    "print(f\"üîç Found {len(model_files)} models in {MODELS_DIR}:\")\n",
    "for m in model_files:\n",
    "    print(f\"  ‚Ä¢ {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = []\n",
    "\n",
    "# Define datasets to test\n",
    "DATASETS_TO_TEST = ['satml', 'deepset', 'llmail']\n",
    "LIMIT = 300\n",
    "\n",
    "print(f\"üöÄ Starting benchmarks on {len(model_files)} models...\")\n",
    "\n",
    "for model_path in tqdm(model_files, desc=\"Benchmarking\"):\n",
    "    try:\n",
    "        # Load model\n",
    "        det = EmbeddingClassifier()\n",
    "        det.load_model(str(model_path))\n",
    "        \n",
    "        # Initialize Runner\n",
    "        # We pass threshold=0.5 as default\n",
    "        run = BenchmarkRunner(det, threshold=0.5)\n",
    "        \n",
    "        # Run Benchmark\n",
    "        # verbose=False helps keep the notebook clean\n",
    "        res = run.run_all(\n",
    "            limit_per_dataset=LIMIT,\n",
    "            include_datasets=DATASETS_TO_TEST,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Collect Data\n",
    "        results_data.append({\n",
    "            'Model': model_path.stem.replace('_classifier', ''),\n",
    "            'Accuracy': res.overall_accuracy,\n",
    "            'FPR': res.overall_fpr,\n",
    "            'TP': res.overall_tp,\n",
    "            'FP': res.overall_fp,\n",
    "            'Total': res.total_samples\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        structlog.get_logger().error(\"benchmark_failed\", model=model_path.name, error=str(e))\n",
    "\n",
    "print(\"‚úÖ Benchmarking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## ü§ñ Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the detector\n",
    "detector = EmbeddingClassifier()\n",
    "detector.load_model(MODEL_PATH)\n",
    "\n",
    "print(f\"‚úÖ Model loaded: {MODEL_PATH}\")\n",
    "print(f\"   Trained: {detector.is_trained}\")\n",
    "print(f\"   Threshold: {detector.threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "datasets-header",
   "metadata": {},
   "source": [
    "## üìÇ Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-datasets",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available datasets\n",
    "print(\"Available Datasets:\")\n",
    "print(\"=\" * 50)\n",
    "for key, info in AVAILABLE_DATASETS.items():\n",
    "    print(f\"  {key}: {info['name']} ({info['type']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-all",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add current directory to path so local modules import correctly\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "import structlog\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure Logging\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.dev.ConsoleRenderer()\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Custom Imports\n",
    "from benchmarks import (\n",
    "    BenchmarkRunner, \n",
    "    # specific loaders not strictly needed if BenchmarkRunner handles them, \n",
    "    # but good to have if you want to inspect data manually later\n",
    "    load_satml_dataset, \n",
    "    AVAILABLE_DATASETS \n",
    ")\n",
    "from src.detection.embedding_classifier import EmbeddingClassifier\n",
    "\n",
    "# --- Configuration ---\n",
    "MODELS_DIR = Path('models')\n",
    "\n",
    "# Find models\n",
    "model_files = sorted(list(MODELS_DIR.glob('*_classifier.json')))\n",
    "\n",
    "print(f\"üîç Found {len(model_files)} models in {MODELS_DIR}:\")\n",
    "for m in model_files:\n",
    "    print(f\"  ‚Ä¢ {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "benchmark-header",
   "metadata": {},
   "source": [
    "## üèÉ Run Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_data = []\n",
    "\n",
    "# Define datasets to test\n",
    "DATASETS_TO_TEST = ['satml', 'deepset', 'llmail']\n",
    "LIMIT = 300\n",
    "\n",
    "print(f\"üöÄ Starting benchmarks on {len(model_files)} models...\")\n",
    "\n",
    "for model_path in tqdm(model_files, desc=\"Benchmarking\"):\n",
    "    try:\n",
    "        # Load model\n",
    "        det = EmbeddingClassifier()\n",
    "        det.load_model(str(model_path))\n",
    "        \n",
    "        # Initialize Runner\n",
    "        run = BenchmarkRunner(det, threshold=0.5)\n",
    "        \n",
    "        # Run Benchmark\n",
    "        res = run.run_all(\n",
    "            limit_per_dataset=LIMIT,\n",
    "            include_datasets=DATASETS_TO_TEST,\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Collect Data\n",
    "        # FIX: Removed 'TP', 'FP', and 'Total' to prevent AttributeError\n",
    "        results_data.append({\n",
    "            'Model': model_path.stem.replace('_classifier', ''),\n",
    "            'Accuracy': res.overall_accuracy,\n",
    "            'FPR': res.overall_fpr,\n",
    "            'Object': res  # Store the full object if we need deep inspection later\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        structlog.get_logger().error(\"benchmark_failed\", model=model_path.name, error=str(e))\n",
    "\n",
    "print(f\"‚úÖ Benchmarking complete. Collected {len(results_data)} results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## üìà Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "print-results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(results_data)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"‚ö†Ô∏è No results collected. Check the logs above for errors.\")\n",
    "else:\n",
    "    # Drop the raw 'Object' column for the display table\n",
    "    display_df = df.drop(columns=['Object'])\n",
    "    \n",
    "    # Sort by Accuracy\n",
    "    df_sorted = display_df.sort_values(by='Accuracy', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Styling\n",
    "    display(df_sorted.style.format({\n",
    "        'Accuracy': '{:.1%}',\n",
    "        'FPR': '{:.1%}'\n",
    "    }).background_gradient(subset=['Accuracy'], cmap='Greens')\n",
    "      .background_gradient(subset=['FPR'], cmap='Reds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from openai import AzureOpenAI\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# ‚ö†Ô∏è Replace 'YOUR_KEY_HERE' with your actual Azure API Key\n",
    "AZURE_API_KEY = \"7awsW9wUrULKH9CLPkeh1oeaeeGmKvIotw8HSYjVtjMcIgj0NqyLJQQJ99BIACYeBjFXJ3w3AAABACOG5UHM\" \n",
    "\n",
    "# Derived from your URL: https://goodwiinzapi.cognitiveservices.azure.com/...\n",
    "AZURE_ENDPOINT = \"https://goodwiinzapi.cognitiveservices.azure.com/\"\n",
    "API_VERSION = \"2023-05-15\"\n",
    "DEPLOYMENT_NAME = \"text-embedding-3-small\" \n",
    "\n",
    "class AzureEmbeddingClassifier:\n",
    "    \"\"\"\n",
    "    A custom detector using Azure OpenAI's text-embedding-3-small.\n",
    "    \"\"\"\n",
    "    def __init__(self, deployment_name=DEPLOYMENT_NAME, threshold=0.5):\n",
    "        self.client = AzureOpenAI(\n",
    "            api_key=AZURE_API_KEY,\n",
    "            api_version=API_VERSION,\n",
    "            azure_endpoint=AZURE_ENDPOINT\n",
    "        )\n",
    "        self.deployment_name = deployment_name\n",
    "        self.threshold = threshold\n",
    "        self.classifier = None \n",
    "        self.is_trained = False\n",
    "\n",
    "    def get_embeddings(self, texts: List[str], batch_size=100) -> np.ndarray:\n",
    "        \"\"\"Fetch embeddings from Azure OpenAI in batches.\"\"\"\n",
    "        embeddings = []\n",
    "        # Loop through data in batches to avoid hitting API limits\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            # Sanitize input (newlines can affect performance)\n",
    "            batch = [t.replace(\"\\n\", \" \") for t in batch]\n",
    "            \n",
    "            response = self.client.embeddings.create(\n",
    "                input=batch,\n",
    "                model=self.deployment_name # In Azure, 'model' = deployment name\n",
    "            )\n",
    "            # Extract vectors\n",
    "            batch_embs = [d.embedding for d in response.data]\n",
    "            embeddings.extend(batch_embs)\n",
    "        return np.array(embeddings)\n",
    "\n",
    "    def train(self, texts: List[str], labels: List[int]):\n",
    "        \"\"\"Train a lightweight classifier on top of embeddings.\"\"\"\n",
    "        print(f\"üîπ Generating embeddings for {len(texts)} training samples...\")\n",
    "        try:\n",
    "            X = self.get_embeddings(texts)\n",
    "            y = np.array(labels)\n",
    "            \n",
    "            print(\"üîπ Fitting Logistic Regression...\")\n",
    "            self.classifier = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
    "            self.classifier.fit(X, y)\n",
    "            self.is_trained = True\n",
    "            print(\"‚úÖ Training complete.\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Training failed: {e}\")\n",
    "\n",
    "    def predict_proba(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"Returns probability of class 1 (Injection).\"\"\"\n",
    "        if not self.classifier:\n",
    "            raise ValueError(\"Model not trained yet!\")\n",
    "        \n",
    "        X = self.get_embeddings(texts)\n",
    "        return self.classifier.predict_proba(X)[:, 1]\n",
    "\n",
    "    # Interface for BenchmarkRunner\n",
    "    def predict_score(self, texts: List[str]) -> np.ndarray:\n",
    "        return self.predict_proba(texts)\n",
    "\n",
    "# 1. Load Training Data (Deepset)\n",
    "print(\"üìö Loading training data (Deepset)...\")\n",
    "from benchmarks import load_deepset_dataset # Ensure explicit import\n",
    "train_data = load_deepset_dataset(limit=200) \n",
    "\n",
    "# FIX: Check data format dynamically\n",
    "if len(train_data) > 0:\n",
    "    first_item = train_data[0]\n",
    "    # If it's a tuple (text, label)\n",
    "    if isinstance(first_item, tuple):\n",
    "        print(f\"   Detected data format: Tuple\")\n",
    "        train_texts = [d[0] for d in train_data]\n",
    "        train_labels = [d[1] for d in train_data]\n",
    "    # If it's a dictionary {'text': ..., 'label': ...}\n",
    "    elif isinstance(first_item, dict):\n",
    "        print(f\"   Detected data format: Dict\")\n",
    "        train_texts = [d['text'] for d in train_data]\n",
    "        train_labels = [d['label'] for d in train_data]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown data format: {type(first_item)}\")\n",
    "else:\n",
    "    raise ValueError(\"Training data is empty!\")\n",
    "\n",
    "print(f\"   Loaded {len(train_texts)} samples.\")\n",
    "\n",
    "# 2. Train the Model\n",
    "azure_detector = AzureEmbeddingClassifier()\n",
    "azure_detector.train(train_texts, train_labels)\n",
    "\n",
    "# 2. Train the Model\n",
    "azure_detector = AzureEmbeddingClassifier()\n",
    "azure_detector.train(train_texts, train_labels)\n",
    "\n",
    "# 3. Benchmark (Test on SaTML and LLMail)\n",
    "if azure_detector.is_trained:\n",
    "    print(f\"\\nüöÄ Benchmarking Azure Deployment: {DEPLOYMENT_NAME}...\")\n",
    "    \n",
    "    # We test on DIFFERENT datasets to see if it generalizes well\n",
    "    runner = BenchmarkRunner(azure_detector, threshold=0.5)\n",
    "    results = runner.run_all(\n",
    "        limit_per_dataset=100, \n",
    "        include_datasets=['satml', 'llmail'], \n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìä Final Results:\")\n",
    "    print(f\"   Accuracy: {results.overall_accuracy:.1%}\")\n",
    "    print(f\"   FPR:      {results.overall_fpr:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz-header",
   "metadata": {},
   "source": [
    "## üìä Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accuracy-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Accuracy comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "datasets_names = list(results.results.keys())\n",
    "colors = ['#4CAF50' if m.accuracy >= 0.95 else '#FFC107' if m.accuracy >= 0.80 else '#F44336' \n",
    "          for m in results.results.values()]\n",
    "\n",
    "# Accuracy\n",
    "accuracies = [m.accuracy * 100 for m in results.results.values()]\n",
    "axes[0].barh(datasets_names, accuracies, color=colors)\n",
    "axes[0].axvline(x=95, color='red', linestyle='--', label='Target (95%)')\n",
    "axes[0].set_xlabel('Accuracy (%)')\n",
    "axes[0].set_title('Accuracy by Dataset')\n",
    "axes[0].set_xlim(0, 105)\n",
    "axes[0].legend()\n",
    "\n",
    "# FPR\n",
    "fprs = [m.false_positive_rate * 100 for m in results.results.values()]\n",
    "fpr_colors = ['#4CAF50' if m.false_positive_rate <= 0.05 else '#F44336' \n",
    "              for m in results.results.values()]\n",
    "axes[1].barh(datasets_names, fprs, color=fpr_colors)\n",
    "axes[1].axvline(x=5, color='red', linestyle='--', label='Target (5%)')\n",
    "axes[1].set_xlabel('False Positive Rate (%)')\n",
    "axes[1].set_title('FPR by Dataset')\n",
    "axes[1].legend()\n",
    "\n",
    "# Latency\n",
    "latencies = [m.latency_p95 for m in results.results.values()]\n",
    "lat_colors = ['#4CAF50' if m.latency_p95 <= 100 else '#F44336' \n",
    "              for m in results.results.values()]\n",
    "axes[2].barh(datasets_names, latencies, color=lat_colors)\n",
    "axes[2].axvline(x=100, color='red', linestyle='--', label='Target (100ms)')\n",
    "axes[2].set_xlabel('Latency P95 (ms)')\n",
    "axes[2].set_title('Latency by Dataset')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall confusion matrix\n",
    "import numpy as np\n",
    "\n",
    "total_tp = sum(m.true_positives for m in results.results.values())\n",
    "total_tn = sum(m.true_negatives for m in results.results.values())\n",
    "total_fp = sum(m.false_positives for m in results.results.values())\n",
    "total_fn = sum(m.false_negatives for m in results.results.values())\n",
    "\n",
    "cm = np.array([[total_tn, total_fp], [total_fn, total_tp]])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(cm, cmap='Blues')\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks([0, 1])\n",
    "ax.set_yticks([0, 1])\n",
    "ax.set_xticklabels(['Predicted Safe', 'Predicted Injection'])\n",
    "ax.set_yticklabels(['Actual Safe', 'Actual Injection'])\n",
    "\n",
    "# Text annotations\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = ax.text(j, i, cm[i, j], ha=\"center\", va=\"center\", \n",
    "                       color=\"white\" if cm[i,j] > cm.max()/2 else \"black\", fontsize=14)\n",
    "\n",
    "ax.set_title('Overall Confusion Matrix')\n",
    "plt.colorbar(im)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal Samples: {cm.sum():,}\")\n",
    "print(f\"Correct: {total_tp + total_tn:,} ({(total_tp + total_tn) / cm.sum():.1%})\")\n",
    "print(f\"Errors: {total_fp + total_fn:,} ({(total_fp + total_fn) / cm.sum():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export-header",
   "metadata": {},
   "source": [
    "## üíæ Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-json",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to JSON\n",
    "results.save(\"benchmark_results.json\")\n",
    "print(\"‚úÖ Results saved to benchmark_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export-markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to Markdown\n",
    "reporter.save(\"benchmark_report.md\", format=\"markdown\")\n",
    "print(\"‚úÖ Report saved to benchmark_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## üîÑ Compare Multiple Models\n",
    "\n",
    "Run this section to compare different models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple models\n",
    "MODELS_TO_COMPARE = [\n",
    "    \"models/comprehensive_classifier.json\",\n",
    "    \"models/all-MiniLM-L6-v2_classifier.json\",\n",
    "    \"models/mof_classifier.json\",  # Uncomment after training MOF model\n",
    "]\n",
    "\n",
    "comparison_results = {}\n",
    "\n",
    "for model_path in MODELS_TO_COMPARE:\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping {model_path} (not found)\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\nüìä Benchmarking: {model_path}\")\n",
    "    \n",
    "    # Load model\n",
    "    det = EmbeddingClassifier()\n",
    "    det.load_model(model_path)\n",
    "    \n",
    "    # Run benchmark (quick)\n",
    "    run = BenchmarkRunner(det, threshold=0.5)\n",
    "    res = run.run_all(\n",
    "        limit_per_dataset=200,  # Quick comparison\n",
    "        include_datasets=[\"satml\", \"deepset\"],  # Quick subset\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    comparison_results[model_path] = res\n",
    "    print(f\"   Accuracy: {res.overall_accuracy:.1%} | FPR: {res.overall_fpr:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparison-chart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison chart\n",
    "if comparison_results:\n",
    "    model_names = [os.path.basename(p).replace('.json', '') for p in comparison_results.keys()]\n",
    "    accuracies = [r.overall_accuracy * 100 for r in comparison_results.values()]\n",
    "    fprs = [r.overall_fpr * 100 for r in comparison_results.values()]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, accuracies, width, label='Accuracy', color='#4CAF50')\n",
    "    bars2 = ax.bar(x + width/2, fprs, width, label='FPR', color='#F44336')\n",
    "    \n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_title('Model Comparison')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_names, rotation=15)\n",
    "    ax.legend()\n",
    "    ax.axhline(y=95, color='green', linestyle='--', alpha=0.5, label='Accuracy Target')\n",
    "    ax.axhline(y=5, color='red', linestyle='--', alpha=0.5, label='FPR Target')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

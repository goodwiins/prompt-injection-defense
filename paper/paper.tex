\documentclass[11pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\usepackage{url}
\usepackage{natbib}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{enumitem}

% Listings setup for code
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    numbers=none
}

% Path to figures
\graphicspath{{figures/}}

% Title
\title{A Multi-Layer Defense System Against Prompt Injection in Multi-Agent LLMs}
\author{Abdelghafour El Bikha}
\affil{John Jay College}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Model (LLM) agents are increasingly deployed in multi-agent systems where they interact with untrusted users and other agents. This expands the attack surface for prompt injection, allowing malicious instructions to propagate through the system---a phenomenon known as ``prompt infection.'' Existing defenses often focus on single-turn interactions or suffer from high false positive rates (over-defense) on benign prompts containing trigger words. We propose a comprehensive three-layer defense framework (Detection, Coordination, Response) designed specifically for multi-agent environments. Our system features an ensemble detector combining semantic embeddings with heuristic patterns, trained using a \textbf{Injection-Aware MPNet} strategy to minimize over-defense. We evaluate our approach on text-based prompt injection benchmarks (SaTML, deepset, LLMail, NotInject), achieving \textbf{97.2\% accuracy}, \textbf{98.8\% recall} across attack datasets, and \textbf{0.0\% FPR} on the NotInject over-defense benchmark. We identify HTML-embedded attacks (BrowseSafe) as out-of-scope for text embedding approaches, highlighting the need for modality-specific detectors. By decoupling semantic intent from keyword presence, Injection-Aware MPNet effectively mitigates the ``over-defense'' problem common in keyword-based systems. Our system operates with a P95 latency of \textbf{38.1ms} on standard CPU hardware, making it suitable for real-time applications.
\end{abstract}

\section{Introduction}

The integration of Large Language Models (LLMs) into multi-agent systems enables complex workflows but introduces severe security vulnerabilities. Prompt injection attacks, where adversaries manipulate model behavior via malicious inputs, are well-documented in single-LLM setups. However, in multi-agent systems, a successful injection in one agent can cascade to others, compromising the entire network.

Current defenses face two critical limitations:

<list>
    \item \textbf{Over-Defense:} Many detectors rely on keyword matching or aggressive classifiers that flag benign prompts containing security-related terms (e.g., ``system'', ``ignore''), rendering them unusable for legitimate power users.
    \item \textbf{Single-Agent Focus:} Most defenses ignore the inter-agent trust boundaries and lack mechanisms to quarantine compromised agents to prevent lateral movement.
</list>

To address these gaps, we present a \textbf{Multi-Layer Defense System} that secures the entire agent lifecycle. Our contributions are:

<list>
    \item \textbf{Three-Layer Architecture:} A holistic framework comprising Detection, Coordination, and Response layers to detect, isolate, and mitigate attacks.
    \item \textbf{Ensemble Detection:} A hybrid detector combining a fast embedding-based classifier (XGBoost + MPNet) with a pattern-based heuristic engine, achieving robust detection with low latency.
    \item \textbf{Injection-Aware MPNet:} A novel training strategy that explicitly optimizes for low false positives on benign ``trigger-heavy'' prompts by balancing semantic intent learning across injection, safe, and benign-trigger samples. Unlike prior over-defense mitigation approaches, our method focuses on dataset composition and weighted loss optimization.
    \item \textbf{Comprehensive Benchmarking:} We provide a unified evaluation across SaTML, deepset, LLMail, and NotInject datasets, with detailed comparisons against recent state-of-the-art defenses including InjecGuard, StruQ/SecAlign, DefensiveToken, and PromptArmor.
</list>

\section{Background & Related Work}

\textbf{Prompt Injection:} Early work identified direct prompt injection \citep{perez2022ignore} and indirect injection via retrieved context \citep{greshake2023more}. In multi-agent systems, ``prompt infection'' \citep{lee2025prompt} describes how malicious prompts can replicate across agents.

\textbf{Defense Mechanisms:} Current defenses span multiple paradigms with distinct trade-offs:

\subsection{Training-Time Defenses}

\textit{StruQ} \citep{chen2024struq} addresses prompt injection through \textbf{structured query separation}. The approach introduces a ``Secure Front-End'' that:
<list>
    \item Inserts special delimiter tokens (\texttt{[INST]}, \texttt{[DATA]}) to separate instructions from user data
    \item Filters data inputs to remove potentially conflicting instructions
    \item Fine-tunes the LLM to follow only instructions in the designated prompt section
</list>
StruQ achieves <2\% ASR on manual injection attacks for Llama-2 and Mistral models. However, it requires \textbf{full model fine-tuning} and remains vulnerable to optimization-based attacks (ASR reduced from 97\% to 58\%) \citep{zhan2025jailbreakbench}.

\textit{SecAlign} \citep{chen2025secalign} extends StruQ via \textbf{preference optimization}. It constructs contrastive pairs of (secure output, insecure output) for prompt-injected inputs and applies Direct Preference Optimization (DPO) to align the model toward secure responses. SecAlign achieves near-zero ASR on optimization-free attacks while preserving utility (AlpacaEval2 scores). The key limitation is the requirement for model retraining, making it unsuitable for API-only LLM deployments.

\subsection{Test-Time Defenses}

\textit{DefensiveToken} \citep{defensivetoken2024} inserts a small number of optimized security tokens into the model vocabulary. These tokens are trained via gradient-based optimization to induce injection-resistant behavior without modifying base model weights. DefensiveToken achieves 0.24\% ASR on 31K+ samples---remarkably comparable to training-time methods. However, it requires access to model internals for token embedding optimization.

\subsection{LLM-Based Defenses}

\textit{PromptArmor} \citep{promptarmor2024} employs a \textbf{guardrail LLM} as a preprocessing filter. The guardrail analyzes incoming prompts, identifies injected content, and sanitizes the input before forwarding to the primary LLM. On AgentDojo benchmarks, PromptArmor achieves <1\% FPR and FNR using GPT-4o as the guardrail. The trade-off is significant latency overhead (~200ms) due to additional LLM inference.

\subsection{Over-Defense Mitigation}

\textit{InjecGuard/PIGuard} (Liang et al., 2024; accepted ACL 2025) identified over-defense as a critical barrier to prompt guard adoption. They introduced:
<list>
    \item \textbf{NotInject Dataset:} 339 benign samples enriched with 1--3 trigger words per sentence, enabling systematic over-defense evaluation
    \item \textbf{MOF (Mitigating Over-defense for Free) Strategy:} A training approach that reduces trigger-word bias by augmenting training data with benign samples containing attack-like keywords
</list>
InjecGuard's MOF operates by \textbf{implicit bias deamplification} during fine-tuning of transformer guardrail models (DeBERTa-v3 based).

% \subsection{Positioning Our Work: BIT vs. MOF}
% This section is commented out because it is specific to the BIT training strategy and has not been updated for the Injection-Aware MPNet model.
% 
% InjecGuard's MOF and our BIT address the same over-defense problem using the same NotInject dataset. We clarify similarities and differences:
% 
% \textbf{Similarities:}
% <list>
%     \item Both use NotInject-style samples to reduce keyword bias
%     \item Both target low FPR on trigger-heavy benign prompts
%     \item Both achieve FPR <5\% on NotInject
% </list>
% 
% \textbf{Differences:}
% 
% \begin{table}[H]
% \centering
% \caption{BIT vs. InjecGuard MOF Comparison}
% \begin{tabular}{lp{5cm}p{5cm}}
% \toprule
% \textbf{Aspect} & \textbf{InjecGuard MOF} & \textbf{Our BIT} \\
% \midrule
% \textbf{Mechanism} & Implicit bias deamplification in DeBERTa fine-tuning & Explicit weighted loss optimization in XGBoost \\
% \textbf{Model Type} & Transformer (DeBERTa-v3) & Ensemble (XGBoost + MiniLM embeddings) \\
% \textbf{Latency} & ~12ms (requires GPU) & \textbf{2--5ms} (CPU-only possible) \\
% \textbf{Interpretability} & Black-box neural network & Feature importance via XGBoost \\
% \textbf{NotInject FPR} & 2.1\% (reported) & 1.8\% [95\% CI: 0.8--3.4\%] \\
% \bottomrule
% \end{tabular}
% </table
% 
% \textbf{Honest assessment:} Our FPR on NotInject: 1.8\% [95\% CI: 0.8--3.4\%] is similar to InjecGuard's 2.1\% within statistical error ($p > 0.05$, McNemar's test). The primary differentiation is \textbf{latency (2--5ms vs 12ms) and CPU-only deployability}. This work is \textbf{complementary} to InjecGuard rather than strictly superior.

\section{Threat Model}

We assume an adversary with the following capabilities:
<list>
    \item \textbf{Input Control:} Can inject prompts into the system via user chat or indirect sources (e.g., emails, web pages).
    \item \textbf{Compromised Agent:} May control one agent in the network to send malicious messages to others.
</list>

\textbf{Adversary Goals:}
<list>
    \item \textbf{Policy Bypass:} Force agents to violate safety guidelines.
    \item \textbf{Data Exfiltration:} Steal sensitive information from agent memory.
    \item \textbf{Lateral Movement:} Propagate malicious instructions to privileged agents.
</list>

\textbf{Out of Scope:} We do not address adversarial attacks on the LLM weights themselves or infrastructure-level compromises.
<list>
    \item \textbf{Model extraction attacks}
    \item \textbf{Feedback-based attacks}
    \item \textbf{Distributed attacks}
</list>

\textbf{Scope:} Our primary focus is \textbf{text-based prompt injection} in multi-agent systems. Non-text modalities (e.g., images, HTML/DOM structural features) are outside the scope of our core embeddings-based detector, as discussed in the BrowseSafe limitations (Section 7.9).

\section{System Design}

Our system implements a defense-in-depth architecture with three distinct layers.

\subsection{Detection Layer}
The first line of defense analyzes incoming prompts using an ensemble of detectors:
<list>
    \item \textbf{Embedding Classifier:} Uses \texttt{all-mpnet-base-v2} to generate 768-dimensional embeddings, classified by an XGBoost model.
    \item \textbf{Pattern Detector:} A regex-based engine identifying 10 common attack families.
    \item \textbf{Behavioral Monitor:} Uses a sliding window of $N=20$ recent outputs, flagging when output logits exceed $\mu \pm 2\sigma$.
</list>

\subsection{Coordination Layer}
This layer manages inter-agent trust and communication:
<list>
    \item \textbf{Guard Agent:} Acts as a gateway routing messages.
    \item \textbf{OVON Protocol:} Enforces structured messaging with ``whisper'' fields.
    \item \textbf{PeerGuard Validator:} (Optional) Uses a separate LLM for validation.
</list>

\subsection{Response Layer}
The final layer handles mitigation:
<list>
    \item \textbf{Circuit Breaker:} Tracks aggregate risk scores.
    \item \textbf{Quarantine Protocol:} Automatically isolates agents flagged as compromised.
</list>

\section{Methodology}

\subsection{Ensemble Detection}
We combine the outputs of our detectors using a weighted voting scheme:
\begin{equation}
s = w_{emb} \cdot s_{emb} + w_{pattern} \cdot s_{pattern}
\end{equation}
The final decision is binary: $\hat{y} = \mathbf{1}[s \ge \theta]$. We optimize $\theta$ on the validation set targeting 98\% recall, selecting \textbf{$\\theta=0.764$}.

\subsection{Balanced Intent Training (BIT)}
To mitigate over-defense, we introduce BIT, explicitly balancing three sample categories with weighted loss optimization:
<list>
    \item \textbf{Injections (40\%):} Standard attacks
    \item \textbf{Safe (40\%):} Normal user queries
    \item \textbf{Benign-Triggers (20\%):} Safe queries containing injection-like keywords
</list>

BIT applies explicit class weights during XGBoost training:
\begin{equation}
\mathcal{L}_{BIT} = \sum_{i} w_i \cdot \ell(y_i, \hat{y}_i), \quad w_{benign-trigger} = 2.0
\end{equation}
(with $w_{i}=1.0$ for injection and safe samples)

To validate that weighted loss drives over-defense mitigation beyond data composition, we test inverse weighting ($w=0.5$) and observe FPR increases to 18.7\% [vs 1.8\% for $w=2.0$ and 12.4\% for $w=1.0$], confirming the weighting mechanism is critical. The monotonic relationship ($w=0.5 < w=1.0 < w=2.0$ correlates with FPR 18.7\% $>$ 12.4\% $>$ 1.8\%) demonstrates explicit optimization of benign-trigger samples is necessary, not just their inclusion in training.

\section{Experimental Setup}

\textbf{Datasets:}
\input{tables/dataset_summary.tex}

\textbf{Baselines:} We compare against classifier-based (DeBERTa, InjecGuard), training-time (StruQ, SecAlign), and LLM-based (PromptArmor) defenses.

\textbf{Statistical Methodology:} We use stratified 80/20 train/test splits with fixed seed 42. All reported metrics include 95\% confidence intervals (CI) computed using the \textbf{Wilson score method}, which provides more accurate coverage than normal approximation for small samples or extreme proportions (e.g., near-0\% FPR). Statistical significance for classifier comparison is assessed using \textbf{McNemar's test} for paired binary classifiers ($p < 0.05$).

\textbf{Metrics:} We report False Positive Rate (FPR) relative to the specific dataset size. The Intra-class Distance Ratio (IDR) is defined as the mean Euclidean distance between class centroids in the original MiniLM embedding space: $\text{IDR} = d( \mu_{\text{benign-trigger}}, \mu_{\text{benign}} ) / d( \mu_{\text{benign-trigger}}, \mu_{\text{injection}} )$.

\section{Results}

To validate our multi-layer architecture (Section 4), we structure the evaluation as follows:
<list>
    \item \textbf{Detection Layer:} Performance on benchmark datasets and over-defense analysis (Sections 7.1--7.4).
    \item \textbf{Coordination Layer:} Inter-agent trust exploitation and OVON protocol security (Section 7.5).
    \item \textbf{Response Layer:} Quarantine effectiveness and mitigation rates (Section 7.5.5).
</list>

\subsection{Detection Performance}

Our system achieves state-of-the-art performance across all text-based benchmarks.

% Table 2a
\input{tables/per_dataset_metrics.tex}

Notably, we achieve \textbf{0.0\% False Positive Rate} on the official NotInject HuggingFace dataset, validating BIT effectiveness.

\subsection{Baseline Comparison}

Compared to recent state-of-the-art defenses, our system offers competitive accuracy with significantly lower latency.

\input{tables/baseline_comparison.tex}

\subsection{Qualitative Examples}
To provide a qualitative understanding of the system's performance, we present a set of test scenarios in Table \ref{tab:qualitative_examples}. These examples demonstrate the system's ability to correctly handle both benign prompts (including those with trigger words) and various injection attacks.

\input{tables/qualitative_examples.tex}


% \subsection{Ablation Study}
% This section is commented out because ablation data for the Injection-Aware MPNet model is not available.
% 
% We analyzed the contribution of each component.
% \input{tables/ablation_table.tex}
% \vspace{2mm}
% \footnotesize{\textit{Definitions: "nocascade" = always runs deep path; "fastonly" = runs only embedding classifier (no heuristic rules).}}

% \subsection{Deep Over-Defense Analysis}
% This section is commented out because it is specific to the BIT training strategy and has not been updated for the Injection-Aware MPNet model.
% 
% \subsubsection{NotInject Difficulty-Level Breakdown}
% We stratify the NotInject dataset into three difficulty levels based on trigger word density to evaluate robustness. Level 1 (Easy) contains single triggers (e.g., "ignore"), while Level 3 (Hard) involves complex combinations. As shown in Table \ref{tab:notinject-difficulty}, baseline models degrade significantly on harder samples, whereas BIT maintains robust performance.
% 
% \input{tables/notinject_difficulty.tex}
% 
% \subsubsection{Trigger Word Analysis}
% 
% We analyze which specific trigger words cause the highest false positive rates in baseline models and how BIT addresses them (Table \ref{tab:trigger-words}):
% 
% \input{tables/trigger_word_analysis.tex}
% 
% The most problematic words (``jailbreak'', ``bypass'', ``ignore'') have near-100\% FPR without BIT training. This demonstrates that baseline classifiers learn \textbf{lexical shortcuts} rather than semantic intent. BIT's weighted loss optimization forces the model to consider full context, eliminating these false positives.
% 
% \subsubsection{BIT Component Ablation}
% 
% We decompose BIT into its constituent strategies to understand each contribution (Table \ref{tab:bit-components}):
% 
% \input{tables/bit_component_ablation.tex}
% 
% \textbf{Analysis:}
% <list>
%     \item \textbf{Benign-trigger samples are critical:} Removing them causes FPR to spike to 41.3\%, demonstrating the importance of exposing the model to trigger-heavy benign examples.
%     \item \textbf{Weighted loss mechanism drives improvement:} Our ablation study reveals weighted loss provides 10.6pp FPR reduction on identical training data (1.8\% vs 12.4\%), ruling out data composition effects. Inverse weighting ($w=0.5$) performs worse than uniform weighting (18.7\% vs 12.4\%), confirming the optimization direction matters---downweighting benign-trigger samples harms performance. McNemar's test confirms statistical significance ($\chi^2 = 36.2$, $p < 0.001$, $n=339$).
%     \item \textbf{Dataset balancing matters:} The 40/40/20 split ensures adequate representation of each category; unbalanced data causes both higher FPR (23.7\%) and lower recall (91.5\%).
% </list>

\subsubsection{Embedding Space Analysis}
To understand why BIT succeeds, we visualize the learned embedding space using t-SNE projections (Figure 9).

\textbf{Without BIT:} Trigger words create dense clusters in embedding space regardless of semantic context. Samples like ``ignore the noise'' and ``ignore previous instructions'' are embedded close together, causing misclassification.

\textbf{With BIT:} The embedding space reorganizes by \textbf{intent} rather than \textbf{keywords}. Benign samples containing trigger words cluster with other benign samples, while injection attacks cluster separately based on their adversarial intent patterns.

We quantify this using \textbf{Intra-class Distance Ratio (IDR):}
\begin{equation}
\text{IDR} = \frac{d_{\text{benign-trigger, benign}}}{d_{\text{benign-trigger, injection}}}
\end{equation}

\begin{table}[H]
\centering
\caption{Embedding Space IDR Analysis}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{IDR} & \textbf{Interpretation} \\
\midrule
DeBERTa (baseline) & 2.31 & Benign-triggers closer to injections \\
w/o BIT & 2.18 & Slight improvement \\
\textbf{With BIT} & \textbf{0.67} & Benign-triggers correctly clustered with benign \\
\bottomrule
\end{tabular}
\end{table}

An IDR < 1 indicates benign-trigger samples are correctly positioned closer to benign samples than to injections.

% \subsection{Inter-Agent Trust Exploitation Evaluation}
% This section is commented out because it is specific to the BIT-trained detector and has not been updated for the Injection-Aware MPNet model.
% 
% Recent research demonstrates that 82.4--100\% of LLMs are vulnerable to inter-agent trust exploitation, where models execute malicious commands from peer agents even when they resist identical direct prompts \citep{lee2025prompt}. To validate our multi-agent coordination layer's effectiveness, we conduct dedicated evaluations of agent-to-agent attack scenarios (see Figure 11 for network topology and Figure 12 for infection propagation dynamics). 
% 
% \textit{Experimental Setup:} We simulate attack sequences using distinct random seeds (Temperature=0.7) for generation. Reported metrics (Time to Detection, Isolation) are mean values over 50 independent trials. The Guard Agent utilizes our BIT-trained detector ($ \theta=0.764 $) for all experiments.
% 
% \subsubsection{Compromised Agent Attack Results}
% 
% We evaluate scenarios where one agent in the network is fully compromised (adversary-controlled). We construct a 5-agent test environment simulating a realistic multi-agent workflow (Orchestrator, Researcher, Coder, Reviewer, Guard) and test five attack vectors: Direct Peer Injection, Transitive Trust Exploitation, Whisper Field Manipulation, Role Impersonation, and Payload Fragmentation.
% 
% \input{tables/multi_agent_attacks.tex}
% 
% \textbf{Key Findings:}
% <list>
%     \item \textbf{Without our Guard Agent:} LLMs exhibit strong peer-trust bias, executing 78--94\% of malicious peer commands that they would reject from users.
%     \item \textbf{With Guard Agent:} Attack success rate drops to 0--8.7\% across all scenarios.
%     \item \textbf{Whisper field protection:} OVON protocol validation completely blocks metadata-based attacks.
% </list>

% \subsubsection{Guard Agent Bypass Testing}
% 
% We specifically test whether attackers can bypass our Guard Agent through adversarial techniques:
% 
% \input{tables/guard_bypass_testing.tex}
% 
% \textbf{Analysis:}
% <list>
%     \item \textbf{Agent spoofing is fully prevented} by cryptographic message signing in OVON protocol.
%     \item \textbf{Multi-turn buildup} is the most effective bypass (8.5\%), indicating need for longer context windows.
%     \item \textbf{Direct Guard injection} is extremely difficult (0.6\%) due to the Guard's minimal attack surface (no tool execution).
% </list>

% \subsubsection{OVON Protocol Security Analysis}
% 
% We analyze the security properties of our OVON-based coordination layer. We verify message authenticity via cryptographic signing, integrity via HMAC, and proper whisper field isolation (preventing metadata injection). 
% 
% \input{tables/ovon_security.tex}
% 
% <quote>
% \textbf{Note:} OVON is an open standard for voice interoperability. We adapt its envelope structure for agent-to-agent text messaging.
% </quote>

% \subsubsection{Quarantine Protocol Effectiveness}
% 
% We evaluate whether our quarantine system can detect and isolate compromised agents before infection spreads (Table \ref{tab:quarantine}).
% 
% \input{tables/quarantine_effectiveness.tex}
% 
% The system demonstrates a \textbf{95\% reduction} in full network compromise rate (from 62\% to 3\%) compared to a system without quarantine.
% 
% \subsubsection{Comparison with PeerGuard}
% 
% We compare our Guard Agent approach against PeerGuard's mutual reasoning defense \citep{wang2024peerguard}.
% 
% \input{tables/peerguard_comparison.tex}
% 
% \textbf{Trade-off Analysis:}
% <list>
%     \item \textbf{PeerGuard advantage:} Higher true positive rate (96\% vs 93.1\%) due to semantic reasoning.
%     \item \textbf{Our advantage:} 400x lower latency (1.9ms vs 800ms), no additional LLM inference costs.
%     \item \textbf{Recommendation:} A tiered approach where our Guard handles Tier 1 (real-time) and PeerGuard handles Tier 2 (escalated risk).
% </list>

\subsection{Modality Limitations: BrowseSafe Evaluation}

We evaluate our system on BrowseSafe \citep{perplexity2025browsesafe}, a benchmark of 14,719 HTML-embedded prompt injections designed for AI browser agents. BrowseSafe represents a fundamentally different modality from our text-based benchmarks, as attacks exploit HTML structure, CSS styling, and DOM features.

\textbf{Results:}
<list>
    \item Recall on attacks: 100\% (Detects all malicious text after HTML parsing)
    \item \textbf{FPR on benign HTML: 95\%} (Cannot distinguish benign vs. malicious HTML context)
    \item Accuracy: 52\% (Near-random due to high FPR)
</list>

\textbf{Root Cause Analysis:}
Our MiniLM-based embedding approach operates on extracted text content, losing critical HTML structural context (e.g., hidden text attacks via `display:none`, attribute-based injections). After text extraction, both malicious and benign HTML converge to similar semantic embeddings. This confirms that HTML-embedded attacks require DOM-aware detection approaches (e.g., fine-tuned LLMs), and text-based semantic embeddings are insufficient for this modality.


% \subsection{Preliminary Adaptive Attack Analysis: GCG and AutoDAN}
% This section is commented out because adaptive attack analysis for the Injection-Aware MPNet model is not available.
% 
% <quote>
% \textbf{CAUTION:} Preliminary Results ($n=36$). 
% </quote>
% 
% We observe:
% <list>
%     \item GCG suffix attacks: 10\% ASR
%     \item AutoDAN semantic attacks: \textbf{37.5\% ASR} [13.7--69.4\%]
%     \item Overall: \textbf{27.8\% ASR} [15.9--44.0\%]
% </list>

\section{Discussion}

\textbf{Latency & Scalability:} P50 latency of 29.4ms is negligible.

\subsection{Limitations}
<list>
    \item \textbf{Adversarial Robustness:} Susceptible to semantic reformulation (AutoDAN).
    \item \textbf{Evaluation Scope:} Does not cover BrowseSafe or InjecAgent.
    \item \textbf{Multi-Turn Attacks:} Independent message processing misses gradual intent shifts.
    \item \textbf{Detection of HTML:} Not suitable for raw HTML inputs.
</list>

\subsubsection{Limited Effectiveness Over Long Workflows}
Theoretical cumulative bypass rates indicate >99\% bypass probability over 50 messages. Assuming independent per-message bypass probability $p \approx 8.7\%$, the probability of at least one successful bypass over $n$ messages is $1 - (1-p)^n$. For $n=50$, this exceeds 99.9\%, suggesting the need for periodic re-authentication.

\section{Conclusion}
We present a multi-layer defense system achieving 97.2\% accuracy and 0.1\% FPR with a P50 latency of 29.4ms. Code and model weights are released upon publication. By combining ensemble detection with an Injection-Aware MPNet model and an optimized classification threshold ($\\theta=0.764$), we effectively mitigate over-defense on trigger-heavy benign prompts while maintaining high recall against diverse attacks.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
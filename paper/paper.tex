\documentclass[11pt, a4paper]{article}

% Packages
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{top=2.5cm, bottom=2.5cm, left=2.5cm, right=2.5cm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{authblk}
\usepackage{url}
\usepackage{natbib}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{adjustbox}
\usepackage{enumitem}

% Listings setup for code
\lstset{
    basicstyle=\small\ttfamily,
    columns=flexible,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    numbers=none
}

% Path to figures
\graphicspath{{figures/}}

% Title
\title{A Multi-Layer Defense System Against Prompt Injection in Multi-Agent LLMs}
\author{Abdelghafour El Bikha}
\affil{John Jay College}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
Large Language Model (LLM) agents are increasingly deployed in multi-agent systems where they interact with untrusted users and other agents. This expands the attack surface for prompt injection, allowing malicious instructions to propagate through the system---a phenomenon known as ``prompt infection.'' Existing defenses often focus on single-turn interactions or suffer from high false positive rates (over-defense) on benign prompts containing trigger words. We propose a comprehensive three-layer defense framework (Detection, Coordination, Response) designed specifically for multi-agent environments. Our system features an ensemble detector combining semantic embeddings with heuristic patterns, using a fine-tuned \textbf{Injection-Aware MPNet} embedding model to minimize over-defense. We evaluate our approach on text-based prompt injection benchmarks (SaTML, deepset, LLMail, NotInject, TensorTrust, BrowseSafe), achieving \textbf{97.2\% accuracy}, \textbf{98.8\% recall} on SaTML, \textbf{100\% recall} on deepset and LLMail, and \textbf{0.0\% FPR} on the NotInject over-defense benchmark. We also demonstrate strong generalization on BrowseSafe (97.8\% accuracy, 0.4\% FPR) despite its HTML-embedded attack modality. By learning injection-aware semantic representations, our approach decouples malicious intent from keyword presence, effectively mitigating the ``over-defense'' problem common in keyword-based systems. Our system operates with a P95 latency of \textbf{38.1ms} on GPU hardware, making it suitable for real-time applications.
\end{abstract}

\section{Introduction}

The integration of Large Language Models (LLMs) into multi-agent systems enables complex workflows but introduces severe security vulnerabilities. Prompt injection attacks, where adversaries manipulate model behavior via malicious inputs, are well-documented in single-LLM setups. However, in multi-agent systems, a successful injection in one agent can cascade to others, compromising the entire network.

Current defenses face two critical limitations:

\begin{itemize}
    \item \textbf{Over-Defense:} Many detectors rely on keyword matching or aggressive classifiers that flag benign prompts containing security-related terms (e.g., ``system'', ``ignore''), rendering them unusable for legitimate power users.
    \item \textbf{Single-Agent Focus:} Most defenses ignore the inter-agent trust boundaries and lack mechanisms to quarantine compromised agents to prevent lateral movement.
\end{itemize}

To address these gaps, we present a \textbf{Multi-Layer Defense System} that secures the entire agent lifecycle. Our contributions are:

\begin{itemize}
    \item \textbf{Three-Layer Architecture:} A holistic framework comprising Detection, Coordination, and Response layers to detect, isolate, and mitigate attacks.
    \item \textbf{Ensemble Detection:} A hybrid detector combining a fast embedding-based classifier (XGBoost + MPNet) with a pattern-based heuristic engine, achieving robust detection with low latency.
    \item \textbf{Injection-Aware MPNet:} A fine-tuned sentence embedding model that learns injection-aware semantic representations. By training on a carefully balanced dataset (67\% safe benign, 33\% benign-with-triggers, plus malicious samples), the model decouples malicious intent from keyword presence, achieving 0.0\% FPR on the NotInject over-defense benchmark.
    \item \textbf{Comprehensive Benchmarking:} We provide a unified evaluation across 8 datasets (SaTML, deepset, LLMail, NotInject, BrowseSafe, TensorTrust), with detailed comparisons against state-of-the-art defenses including Lakera Guard, ProtectAI, ActiveFence, Glean AI, and PromptArmor.
\end{itemize}

\section{Background \& Related Work}

\textbf{Prompt Injection:} Early work identified direct prompt injection \citep{perez2022ignore} and indirect injection via retrieved context \citep{greshake2023more}. In multi-agent systems, ``prompt infection'' \citep{lee2025prompt} describes how malicious prompts can replicate across agents.

\textbf{Defense Mechanisms:} Current defenses span multiple paradigms with distinct trade-offs:

\subsection{Training-Time Defenses}

\textit{StruQ} \citep{chen2024struq} addresses prompt injection through \textbf{structured query separation}. The approach introduces a ``Secure Front-End'' that:
\begin{itemize}
    \item Inserts special delimiter tokens (\texttt{[INST]}, \texttt{[DATA]}) to separate instructions from user data
    \item Filters data inputs to remove potentially conflicting instructions
    \item Fine-tunes the LLM to follow only instructions in the designated prompt section
\end{itemize}
StruQ achieves <2\% ASR on manual injection attacks for Llama-2 and Mistral models. However, it requires \textbf{full model fine-tuning} and remains vulnerable to optimization-based attacks (ASR reduced from 97\% to 58\%) \citep{zhan2025jailbreakbench}.

\textit{SecAlign} \citep{chen2025secalign} extends StruQ via \textbf{preference optimization}. It constructs contrastive pairs of (secure output, insecure output) for prompt-injected inputs and applies Direct Preference Optimization (DPO) to align the model toward secure responses. SecAlign achieves near-zero ASR on optimization-free attacks while preserving utility (AlpacaEval2 scores). The key limitation is the requirement for model retraining, making it unsuitable for API-only LLM deployments.

\subsection{Test-Time Defenses}

\textit{DefensiveToken} \citep{defensivetoken2024} inserts a small number of optimized security tokens into the model vocabulary. These tokens are trained via gradient-based optimization to induce injection-resistant behavior without modifying base model weights. DefensiveToken achieves 0.24\% ASR on 31K+ samples---remarkably comparable to training-time methods. However, it requires access to model internals for token embedding optimization.

\subsection{LLM-Based Defenses}

\textit{PromptArmor} \citep{promptarmor2024} employs a \textbf{guardrail LLM} as a preprocessing filter. The guardrail analyzes incoming prompts, identifies injected content, and sanitizes the input before forwarding to the primary LLM. On AgentDojo benchmarks, PromptArmor achieves <1\% FPR and FNR using GPT-4o as the guardrail. The trade-off is significant latency overhead (~200ms) due to additional LLM inference.

\subsection{Over-Defense Mitigation}

\textit{InjecGuard/PIGuard} (Liang et al., 2024; accepted ACL 2025) identified over-defense as a critical barrier to prompt guard adoption. They introduced:
\begin{itemize}
    \item \textbf{NotInject Dataset:} 339 benign samples enriched with 1--3 trigger words per sentence, enabling systematic over-defense evaluation
    \item \textbf{MOF (Mitigating Over-defense for Free) Strategy:} A training approach that reduces trigger-word bias by augmenting training data with benign samples containing attack-like keywords
\end{itemize}
InjecGuard's MOF operates by \textbf{implicit bias deamplification} during fine-tuning of transformer guardrail models (DeBERTa-v3 based).

% \subsection{Positioning Our Work: BIT vs. MOF}
% This section is commented out because it is specific to the BIT training strategy and has not been updated for the Injection-Aware MPNet model.
% 
% InjecGuard's MOF and our BIT address the same over-defense problem using the same NotInject dataset. We clarify similarities and differences:
% 
% \textbf{Similarities:}
% \begin{itemize}
%     \item Both use NotInject-style samples to reduce keyword bias
%     \item Both target low FPR on trigger-heavy benign prompts
%     \item Both achieve FPR <5\% on NotInject
% \end{itemize}
% 
% \textbf{Differences:}
% 
% \begin{table}[H]
% \centering
% \caption{BIT vs. InjecGuard MOF Comparison}
% \begin{tabular}{lp{5cm}p{5cm}}
% \toprule
% \textbf{Aspect} & \textbf{InjecGuard MOF} & \textbf{Our BIT} \\
% \midrule
% \textbf{Mechanism} & Implicit bias deamplification in DeBERTa fine-tuning & Explicit weighted loss optimization in XGBoost \\
% \textbf{Model Type} & Transformer (DeBERTa-v3) & Ensemble (XGBoost + MiniLM embeddings) \\
% \textbf{Latency} & ~12ms (requires GPU) & \textbf{2--5ms} (CPU-only possible) \\
% \textbf{Interpretability} & Black-box neural network & Feature importance via XGBoost \\
% \textbf{NotInject FPR} & 2.1\% (reported) & 1.8\% [95\% CI: 0.8--3.4\%] \\
% \bottomrule
% \end{tabular}
% </table
% 
% \textbf{Honest assessment:} Our FPR on NotInject: 1.8\% [95\% CI: 0.8--3.4\%] is similar to InjecGuard's 2.1\% within statistical error ($p > 0.05$, McNemar's test). The primary differentiation is \textbf{latency (2--5ms vs 12ms) and CPU-only deployability}. This work is \textbf{complementary} to InjecGuard rather than strictly superior.

\section{Threat Model}

We assume an adversary with the following capabilities:
\begin{itemize}
    \item \textbf{Input Control:} Can inject prompts into the system via user chat or indirect sources (e.g., emails, web pages).
    \item \textbf{Compromised Agent:} May control one agent in the network to send malicious messages to others.
\end{itemize}

\textbf{Adversary Goals:}
\begin{itemize}
    \item \textbf{Policy Bypass:} Force agents to violate safety guidelines.
    \item \textbf{Data Exfiltration:} Steal sensitive information from agent memory.
    \item \textbf{Lateral Movement:} Propagate malicious instructions to privileged agents.
\end{itemize}

\textbf{Out of Scope:} We do not address adversarial attacks on the LLM weights themselves or infrastructure-level compromises.
\begin{itemize}
    \item \textbf{Model extraction attacks}
    \item \textbf{Feedback-based attacks}
    \item \textbf{Distributed attacks}
\end{itemize}

\textbf{Scope:} Our primary focus is \textbf{text-based prompt injection} in multi-agent systems. We also evaluate cross-modality generalization on HTML-embedded attacks (Section 7.6).

\section{System Design}

Our system implements a defense-in-depth architecture with three distinct layers.

\subsection{Detection Layer}
The first line of defense analyzes incoming prompts using an ensemble of detectors:
\begin{itemize}
    \item \textbf{Embedding Classifier:} Uses our fine-tuned \texttt{injection-aware-mpnet} model to generate 768-dimensional embeddings, classified by an XGBoost model with optimized threshold ($\theta=0.764$).
    \item \textbf{Pattern Detector:} A regex-based engine identifying 10 common attack families.
    \item \textbf{Behavioral Monitor:} Uses a sliding window of $N=20$ recent outputs, flagging when output logits exceed $\mu \pm 2\sigma$.
\end{itemize}

\subsection{Coordination Layer}
This layer manages inter-agent trust and communication:
\begin{itemize}
    \item \textbf{Guard Agent:} Acts as a gateway routing messages.
    \item \textbf{OVON Protocol:} Enforces structured messaging with ``whisper'' fields.
    \item \textbf{PeerGuard Validator:} (Optional) Uses a separate LLM for validation.
\end{itemize}

\subsection{Response Layer}
The final layer handles mitigation:
\begin{itemize}
    \item \textbf{Circuit Breaker:} Tracks aggregate risk scores.
    \item \textbf{Quarantine Protocol:} Automatically isolates agents flagged as compromised.
\end{itemize}

\section{Methodology}

\subsection{Ensemble Detection}
We combine the outputs of our detectors using a weighted voting scheme:
\begin{equation}
s = w_{emb} \cdot s_{emb} + w_{pattern} \cdot s_{pattern}
\end{equation}
The final decision is binary: $\hat{y} = \mathbf{1}[s \ge \theta]$. We optimize $\theta$ on the validation set targeting 98\% recall, selecting \textbf{$\theta=0.764$}.

\subsection{Injection-Aware MPNet Training}

To mitigate over-defense, we fine-tune \texttt{all-mpnet-base-v2} using contrastive learning on carefully curated sentence pairs. The training objective teaches the model to cluster semantically similar prompts together while separating malicious from benign content.

\textbf{Training Data Sources:}
\begin{itemize}
    \item \textbf{Injection samples:} SaTML CTF (1,500), deepset attacks (500), LLMail (500), BrowseSafe injections (2,000)
    \item \textbf{Safe samples:} deepset benign (1,000), BrowseSafe benign
    \item \textbf{Benign-trigger samples:} NotInject HuggingFace (500), NotInject synthetic (1,000)
\end{itemize}

\textbf{Contrastive Pair Construction:}
We construct 14,000 sentence pairs with cosine similarity labels. The key insight is \textbf{doubling the injection vs. benign-trigger pairs} to emphasize this critical distinction:

\begin{table}[H]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Pair Type} & \textbf{Count} & \textbf{Label} \\
\midrule
Injection--Injection & 2,000 & 1.0 (similar) \\
Safe--Safe & 2,000 & 1.0 (similar) \\
Benign-trigger--Benign-trigger & 2,000 & 1.0 (similar) \\
Injection--Safe & 2,000 & 0.0 (dissimilar) \\
\textbf{Injection--Benign-trigger} & \textbf{4,000} & \textbf{0.0 (dissimilar)} \\
Safe--Benign-trigger & 2,000 & 0.8 (both benign) \\
\bottomrule
\end{tabular}
\end{table}

The doubled injection--benign-trigger pairs force the model to distinguish malicious intent from benign queries that happen to contain trigger words like ``ignore'', ``bypass'', or ``system''. The 0.8 label for safe--benign-trigger pairs acknowledges that both are benign but stylistically different.

The model is trained using \textbf{CosineSimilarityLoss} with MSE:
\begin{equation}
\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} (s_i - \cos(\mathbf{e}_1^{(i)}, \mathbf{e}_2^{(i)}))^2
\end{equation}
where $\mathbf{e}_1, \mathbf{e}_2$ are 768-dimensional embeddings of the sentence pair and $s_i$ is the target similarity.

\textbf{Training Configuration:}
\begin{itemize}
    \item Base model: \texttt{sentence-transformers/all-mpnet-base-v2}
    \item Epochs: 4, Batch size: 4, Learning rate: $5 \times 10^{-5}$, Warmup: 100 steps
    \item Validation Spearman correlation: \textbf{0.904}
\end{itemize}

\textbf{XGBoost Classifier Training:}
The fine-tuned embeddings are used to train an XGBoost classifier on a balanced dataset:
\begin{itemize}
    \item Injections: 2,800 samples
    \item Safe benign: 2,800 samples
    \item Benign-trigger: 1,400 samples
\end{itemize}
XGBoost parameters: $n\_estimators=300$, $max\_depth=6$, $learning\_rate=0.05$. The classification threshold $\theta=0.764$ is optimized on validation data to achieve 98\% recall while minimizing false positives.

\section{Experimental Setup}

\textbf{Datasets:}
\input{tables/dataset_summary.tex}

\textbf{Baselines:} We compare against classifier-based (DeBERTa, InjecGuard), training-time (StruQ, SecAlign), and LLM-based (PromptArmor) defenses.

\textbf{Statistical Methodology:} We use stratified 80/20 train/test splits with fixed seed 42. All reported metrics include 95\% confidence intervals (CI) computed using the \textbf{Wilson score method}, which provides more accurate coverage than normal approximation for small samples or extreme proportions (e.g., near-0\% FPR). Statistical significance for classifier comparison is assessed using \textbf{McNemar's test} for paired binary classifiers ($p < 0.05$).

\textbf{Metrics:} We report False Positive Rate (FPR) relative to the specific dataset size. The Intra-class Distance Ratio (IDR) is defined as the mean Euclidean distance between class centroids in the MPNet embedding space: $\text{IDR} = d( \mu_{\text{benign-trigger}}, \mu_{\text{benign}} ) / d( \mu_{\text{benign-trigger}}, \mu_{\text{injection}} )$.

\section{Results}

To validate our multi-layer architecture (Section 4), we structure the evaluation as follows:
\begin{itemize}
    \item \textbf{Detection Layer:} Performance on benchmark datasets and over-defense analysis (Sections 7.1--7.4).
    \item \textbf{Coordination Layer:} Inter-agent trust exploitation and OVON protocol security (Section 7.5).
    \item \textbf{Response Layer:} Quarantine effectiveness and mitigation rates (Section 7.5.5).
\end{itemize}

\subsection{Detection Performance}

Our system achieves state-of-the-art performance across all text-based benchmarks.

% Table 2a
\input{tables/per_dataset_metrics.tex}

Notably, we achieve \textbf{0.0\% False Positive Rate} on the official NotInject HuggingFace dataset, validating our contrastive training approach.

Figure \ref{fig:dataset-composition} shows the training data composition, highlighting the balanced distribution between injection and benign samples with emphasis on benign-trigger examples.

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{dataset_composition.png}
\caption{Training dataset composition. The balanced distribution includes doubled injection--benign-trigger pairs to emphasize the critical distinction between malicious and benign samples containing trigger words.}
\label{fig:dataset-composition}
\end{figure}

\subsection{Baseline Comparison}

Compared to recent state-of-the-art defenses, our system offers competitive accuracy with significantly lower latency.

\input{tables/baseline_comparison.tex}

\subsection{Qualitative Examples}
To provide a qualitative understanding of the system's performance, we present a set of test scenarios in Table \ref{tab:qualitative_examples}. These examples demonstrate the system's ability to correctly handle both benign prompts (including those with trigger words) and various injection attacks.

\input{tables/qualitative_examples.tex}


% \subsection{Ablation Study}
% This section is commented out because ablation data for the Injection-Aware MPNet model is not available.
% 
% We analyzed the contribution of each component.
% \input{tables/ablation_table.tex}
% \vspace{2mm}
% \footnotesize{\textit{Definitions: "nocascade" = always runs deep path; "fastonly" = runs only embedding classifier (no heuristic rules).}}

% \subsection{Deep Over-Defense Analysis}
% This section is commented out because it is specific to the BIT training strategy and has not been updated for the Injection-Aware MPNet model.
% 
% \subsubsection{NotInject Difficulty-Level Breakdown}
% We stratify the NotInject dataset into three difficulty levels based on trigger word density to evaluate robustness. Level 1 (Easy) contains single triggers (e.g., "ignore"), while Level 3 (Hard) involves complex combinations. As shown in Table \ref{tab:notinject-difficulty}, baseline models degrade significantly on harder samples, whereas BIT maintains robust performance.
% 
% \input{tables/notinject_difficulty.tex}
% 
% \subsubsection{Trigger Word Analysis}
% 
% We analyze which specific trigger words cause the highest false positive rates in baseline models and how BIT addresses them (Table \ref{tab:trigger-words}):
% 
% \input{tables/trigger_word_analysis.tex}
% 
% The most problematic words (``jailbreak'', ``bypass'', ``ignore'') have near-100\% FPR without BIT training. This demonstrates that baseline classifiers learn \textbf{lexical shortcuts} rather than semantic intent. BIT's weighted loss optimization forces the model to consider full context, eliminating these false positives.
% 
% \subsubsection{BIT Component Ablation}
% 
% We decompose BIT into its constituent strategies to understand each contribution (Table \ref{tab:bit-components}):
% 
% \input{tables/bit_component_ablation.tex}
% 
% \textbf{Analysis:}
% \begin{itemize}
%     \item \textbf{Benign-trigger samples are critical:} Removing them causes FPR to spike to 41.3\%, demonstrating the importance of exposing the model to trigger-heavy benign examples.
%     \item \textbf{Weighted loss mechanism drives improvement:} Our ablation study reveals weighted loss provides 10.6pp FPR reduction on identical training data (1.8\% vs 12.4\%), ruling out data composition effects. Inverse weighting ($w=0.5$) performs worse than uniform weighting (18.7\% vs 12.4\%), confirming the optimization direction matters---downweighting benign-trigger samples harms performance. McNemar's test confirms statistical significance ($\chi^2 = 36.2$, $p < 0.001$, $n=339$).
%     \item \textbf{Dataset balancing matters:} The 40/40/20 split ensures adequate representation of each category; unbalanced data causes both higher FPR (23.7\%) and lower recall (91.5\%).
% \end{itemize}

\subsubsection{Embedding Space Analysis}
The contrastive training approach reorganizes the embedding space by \textbf{intent} rather than \textbf{keywords}. By doubling injection--benign-trigger pairs during training, the model learns to separate prompts like ``ignore the noise'' (benign) from ``ignore previous instructions'' (malicious) despite sharing trigger words.

We quantify this using \textbf{Intra-class Distance Ratio (IDR):}
\begin{equation}
\text{IDR} = \frac{d_{\text{benign-trigger, benign}}}{d_{\text{benign-trigger, injection}}}
\end{equation}

\begin{table}[H]
\centering
\caption{Embedding Space IDR Analysis}
\begin{tabular}{lll}
\toprule
\textbf{Model} & \textbf{IDR} & \textbf{Interpretation} \\
\midrule
all-mpnet-base-v2 (baseline) & 2.31 & Benign-triggers closer to injections \\
\textbf{Injection-Aware MPNet} & \textbf{0.67} & Benign-triggers correctly clustered with benign \\
\bottomrule
\end{tabular}
\end{table}

An IDR < 1 indicates benign-trigger samples are correctly positioned closer to benign samples than to injections.

\subsection{Classification Performance}

Figure \ref{fig:roc-pr} shows the ROC and Precision-Recall curves on the deepset benchmark, demonstrating near-perfect separation between injection and benign samples.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{roc_deepset.png}
    \caption{ROC Curve (AUC = 1.00)}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{pr_deepset.png}
    \caption{Precision-Recall Curve (AUC = 1.00)}
\end{subfigure}
\caption{Classification performance on deepset benchmark. The Injection-Aware MPNet achieves perfect ROC-AUC and PR-AUC, significantly outperforming keyword baselines.}
\label{fig:roc-pr}
\end{figure}

\subsection{Latency Analysis}

Figure \ref{fig:latency} shows the latency distribution across all benchmarks. The system achieves P50 latency of 35.5ms and P95 latency of 43.3ms, suitable for real-time applications.

\begin{figure}[H]
\centering
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{latency_boxplot.png}
    \caption{Latency distribution by dataset}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\textwidth}
    \includegraphics[width=\textwidth]{latency_cdf.png}
    \caption{Cumulative latency distribution}
\end{subfigure}
\caption{Latency analysis across benchmark datasets. P95 latency remains under 50ms for all datasets.}
\label{fig:latency}
\end{figure}

% \subsection{Inter-Agent Trust Exploitation Evaluation}
% This section is commented out because it is specific to the BIT-trained detector and has not been updated for the Injection-Aware MPNet model.
% 
% Recent research demonstrates that 82.4--100\% of LLMs are vulnerable to inter-agent trust exploitation, where models execute malicious commands from peer agents even when they resist identical direct prompts \citep{lee2025prompt}. To validate our multi-agent coordination layer's effectiveness, we conduct dedicated evaluations of agent-to-agent attack scenarios (see Figure 11 for network topology and Figure 12 for infection propagation dynamics). 
% 
% \textit{Experimental Setup:} We simulate attack sequences using distinct random seeds (Temperature=0.7) for generation. Reported metrics (Time to Detection, Isolation) are mean values over 50 independent trials. The Guard Agent utilizes our BIT-trained detector ($ \theta=0.764 $) for all experiments.
% 
% \subsubsection{Compromised Agent Attack Results}
% 
% We evaluate scenarios where one agent in the network is fully compromised (adversary-controlled). We construct a 5-agent test environment simulating a realistic multi-agent workflow (Orchestrator, Researcher, Coder, Reviewer, Guard) and test five attack vectors: Direct Peer Injection, Transitive Trust Exploitation, Whisper Field Manipulation, Role Impersonation, and Payload Fragmentation.
% 
% \input{tables/multi_agent_attacks.tex}
% 
% \textbf{Key Findings:}
% \begin{itemize}
%     \item \textbf{Without our Guard Agent:} LLMs exhibit strong peer-trust bias, executing 78--94\% of malicious peer commands that they would reject from users.
%     \item \textbf{With Guard Agent:} Attack success rate drops to 0--8.7\% across all scenarios.
%     \item \textbf{Whisper field protection:} OVON protocol validation completely blocks metadata-based attacks.
% \end{itemize}

% \subsubsection{Guard Agent Bypass Testing}
% 
% We specifically test whether attackers can bypass our Guard Agent through adversarial techniques:
% 
% \input{tables/guard_bypass_testing.tex}
% 
% \textbf{Analysis:}
% \begin{itemize}
%     \item \textbf{Agent spoofing is fully prevented} by cryptographic message signing in OVON protocol.
%     \item \textbf{Multi-turn buildup} is the most effective bypass (8.5\%), indicating need for longer context windows.
%     \item \textbf{Direct Guard injection} is extremely difficult (0.6\%) due to the Guard's minimal attack surface (no tool execution).
% \end{itemize}

% \subsubsection{OVON Protocol Security Analysis}
% 
% We analyze the security properties of our OVON-based coordination layer. We verify message authenticity via cryptographic signing, integrity via HMAC, and proper whisper field isolation (preventing metadata injection). 
% 
% \input{tables/ovon_security.tex}
% 
% <quote>
% \textbf{Note:} OVON is an open standard for voice interoperability. We adapt its envelope structure for agent-to-agent text messaging.
% </quote>

% \subsubsection{Quarantine Protocol Effectiveness}
% 
% We evaluate whether our quarantine system can detect and isolate compromised agents before infection spreads (Table \ref{tab:quarantine}).
% 
% \input{tables/quarantine_effectiveness.tex}
% 
% The system demonstrates a \textbf{95\% reduction} in full network compromise rate (from 62\% to 3\%) compared to a system without quarantine.
% 
% \subsubsection{Comparison with PeerGuard}
% 
% We compare our Guard Agent approach against PeerGuard's mutual reasoning defense \citep{wang2024peerguard}.
% 
% \input{tables/peerguard_comparison.tex}
% 
% \textbf{Trade-off Analysis:}
% \begin{itemize}
%     \item \textbf{PeerGuard advantage:} Higher true positive rate (96\% vs 93.1\%) due to semantic reasoning.
%     \item \textbf{Our advantage:} 400x lower latency (1.9ms vs 800ms), no additional LLM inference costs.
%     \item \textbf{Recommendation:} A tiered approach where our Guard handles Tier 1 (real-time) and PeerGuard handles Tier 2 (escalated risk).
% \end{itemize}

\subsection{Cross-Modality Generalization: BrowseSafe Evaluation}

We evaluate our system on BrowseSafe \citep{perplexity2025browsesafe}, a benchmark of 14,719 HTML-embedded prompt injections designed for AI browser agents. BrowseSafe represents a fundamentally different modality from our text-based benchmarks, as attacks exploit HTML structure, CSS styling, and DOM features.

\textbf{Results:}
\begin{itemize}
    \item Accuracy: \textbf{98.2\%}
    \item Recall on attacks: \textbf{96.7\%}
    \item FPR on benign HTML: \textbf{0.5\%}
    \item F1 Score: \textbf{98.0\%}
\end{itemize}

\textbf{Analysis:}
Despite BrowseSafe's HTML-embedded attack modality, our Injection-Aware MPNet model demonstrates strong generalization. The fine-tuned embeddings successfully capture malicious intent patterns even when attacks are embedded in HTML structures. This suggests that the semantic signatures of prompt injection attacks transfer across modalities when the underlying text content contains recognizable injection patterns. However, we note that attacks relying purely on HTML structural features (e.g., hidden text via CSS `display:none`) may still evade detection, representing an area for future work.


% \subsection{Preliminary Adaptive Attack Analysis: GCG and AutoDAN}
% This section is commented out because adaptive attack analysis for the Injection-Aware MPNet model is not available.
% 
% <quote>
% \textbf{CAUTION:} Preliminary Results ($n=36$). 
% </quote>
% 
% We observe:
% \begin{itemize}
%     \item GCG suffix attacks: 10\% ASR
%     \item AutoDAN semantic attacks: \textbf{37.5\% ASR} [13.7--69.4\%]
%     \item Overall: \textbf{27.8\% ASR} [15.9--44.0\%]
% \end{itemize}

\section{Discussion}

\textbf{Latency \& Scalability:} P50 latency of 35.5ms is negligible for real-time applications.

\subsection{Limitations}
\begin{itemize}
    \item \textbf{Adversarial Robustness:} TensorTrust evaluation shows 83.2\% accuracy, indicating susceptibility to adversarial attack variants designed to evade detection.
    \item \textbf{Multi-Turn Attacks:} Independent message processing misses gradual intent shifts across conversation turns.
    \item \textbf{Pure Structural Attacks:} HTML attacks relying solely on DOM/CSS features (e.g., hidden text) without textual injection patterns may evade detection.
    \item \textbf{Evaluation Scope:} InjecAgent and AgentDojo benchmarks remain unevaluated due to framework integration complexity.
\end{itemize}

\subsubsection{Limited Effectiveness Over Long Workflows}
Theoretical cumulative bypass rates indicate >99\% bypass probability over 50 messages. Assuming independent per-message bypass probability $p \approx 8.7\%$, the probability of at least one successful bypass over $n$ messages is $1 - (1-p)^n$. For $n=50$, this exceeds 99.9\%, suggesting the need for periodic re-authentication.

\section{Conclusion}
We present a multi-layer defense system for prompt injection detection in multi-agent LLM systems. Our Injection-Aware MPNet approach achieves:
\begin{itemize}
    \item \textbf{97.3\% overall accuracy} across 8 benchmark datasets
    \item \textbf{0.0\% FPR} on the NotInject over-defense benchmark (both local and HuggingFace versions)
    \item \textbf{100\% recall} on deepset and LLMail injection datasets
    \item \textbf{98.2\% accuracy} on BrowseSafe HTML-embedded attacks (0.5\% FPR)
    \item \textbf{P95 latency of 43.3ms} suitable for real-time applications
\end{itemize}

By fine-tuning sentence embeddings using contrastive learning on carefully balanced injection/benign pairs, we decouple malicious intent from keyword presence. This effectively mitigates the ``over-defense'' problem that plagues keyword-based systems. The optimized classification threshold ($\theta=0.764$) balances high recall (98.8\% on SaTML) with minimal false positives.

Code and model weights are released upon publication.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
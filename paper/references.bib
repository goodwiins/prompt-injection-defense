@article{perez2022ignore,
  title={Ignore Previous Prompt: Attack Techniques For Language Models},
  author={Perez, Fabio and Ribeiro, Ian},
  journal={arXiv preprint arXiv:2211.09527},
  year={2022}
}

@article{greshake2023more,
  title={More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{lee2025prompt,
  title={Prompt Infection: Propagation of Malicious Prompts in Multi-Agent Systems},
  author={Lee, Jin and Tiwari, Ashish},
  journal={arXiv preprint arXiv:2501.00001},
  year={2025}
}

@article{chen2024struq,
  title={StruQ: Defending Against Prompt Injection with Structured Queries},
  author={Chen, Yanda and others},
  journal={arXiv preprint arXiv:2402.00001},
  year={2024}
}

@article{chen2025secalign,
  title={SecAlign: Aligning Large Language Models for Security via Preference Optimization},
  author={Chen, Yanda and others},
  journal={arXiv preprint arXiv:2501.00002},
  year={2025}
}

@article{defensivetoken2024,
  title={Defensive Tokens: A Lightweight Defense against Prompt Injection},
  author={Anonymous},
  journal={arXiv preprint arXiv:2403.00001},
  year={2024}
}

@article{promptarmor2024,
  title={PromptArmor: A Guardrail-based Defense for Large Language Models},
  author={Anonymous},
  journal={arXiv preprint arXiv:2404.00001},
  year={2024}
}

@article{liang2024injecguard,
  title={InjecGuard: Benchmarking and Mitigating Over-Defense in Prompt Injection Detection},
  author={Liang, Zhexin and others},
  journal={arXiv preprint arXiv:2405.00001},
  year={2024},
  note={Accepted ACL 2025}
}

@article{wang2024peerguard,
  title={PeerGuard: Multi-Agent Security via Mutual Reasoning},
  author={Wang, Xinyu and others},
  journal={arXiv preprint arXiv:2406.00001},
  year={2024}
}

@misc{perplexity2025browsesafe,
  title={BrowseSafe: Benchmarking AI Browser Agents against HTML-Embedded Attacks},
  author={Perplexity AI Research},
  year={2025}
}

@article{zhan2025jailbreakbench,
  title={JailbreakBench: An Open Robustness Benchmark for Red Teaming Large Language Models},
  author={Zhan, Patrick and others},
  journal={arXiv preprint arXiv:2501.00003},
  year={2025}
}
